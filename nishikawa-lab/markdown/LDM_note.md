# Note (this is only for me to understand the paper)

latentç©ºé–“ã§diffusionã™ã‚‹ã¨ã„ã†ã®ãŒè‚
- è¨ˆç®—é‡ãŒæ¿€æ¸›ã—ã€10~100å€æ—©ããªã‚‹
stable diffusionãªã©ã‚’PCã§å‹•ã‹ã›ã‚‹ç†ç”±

latentæ§‹é€ ã«ã¯**æ„å‘³çš„æ§‹é€ **ãŒã™ã§ã«åœ§ç¸®ã•ã‚Œã¦ã„ã‚‹
autoencoderãŒ
- high-level ãªå½¢çŠ¶
- semantic consistency
- perceptual similarity
ã‚’latentã«æŠ¼ã—è¾¼ã‚ã‚‹ãŸã‚diffusionãŒå­¦ç¿’ã™ã‚‹ã¹ãåˆ†å¸ƒãŒæ‰±ã„ã‚„ã™ããªã‚‹

ç”»åƒä»¥å¤–ã¸ã®å¿œç”¨ãŒå®¹æ˜“
- audio latent
- video latent
- 3D latent
- text latent (in progress)
- text latent (in progress)
é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯latentè¡¨ç¾ã‚’æŒã¤ã®ãŒå¸¸è­˜ã«ãªã‚Šã¤ã¤ã‚ã‚‹

é€†æ–¹å‘ã®ç†è«– (score, æƒ…å ±ä¿å­˜) ãŒç†è§£ã—ã‚„ã™ããªã‚‹
pixelç©ºé–“ã‚ˆã‚Šlatentç©ºé–“ã®æ–¹ãŒç”Ÿæˆã™ã¹ãç¢ºç«‹åˆ†å¸ƒãŒå¹³æ»‘ãªã®ã§socre (âˆ‡logp(x))ã®æ¨å®šãŒå®‰å®š

ä»¥ä¸‹ã¯è«–æ–‡ã®é‡è¦ã ã¨æ€ã£ãŸéƒ¨åˆ†ã‚’æŠœãå‡ºã—ãŸã‚‚ã®

## Abstract

æ—¢å­˜ã®DMsã¯pixelç©ºé–“ã§è¨ˆç®—ã‚’ã—ã¦ã„ãŸã®ã§å¤§é‡ã®GPUã‚’ä½¿ç”¨ã—ã€é€£ç¶šè©•ä¾¡ã«ã‚ˆã‚Šæ¨è«–ã¯é«˜ä¾¡ã ã£ãŸ
LDMã¯ latentç©ºé–“ã§è¨ˆç®—ã‚’ã™ã‚‹ã“ã¨ã§è¤‡é›‘æ€§ã®æ¸›å°‘ã¨è©³ç´°ç¶­æŒãƒ»å¿ å®Ÿæ€§ã®é–“ã®ã»ã¼æœ€é©ã¨è¨€ãˆã‚‹ãƒãƒ©ãƒ³ã‚¹ã«åˆ°é”ã—ãŸ(ã¨ã„ã†è§¦ã‚Œè¾¼ã¿)
- cross attention layer
- covolutional manner
ãªã©ã®å°å…¥ã«ã‚ˆã‚Šå…¨èˆ¬çš„ãªæ¡ä»¶ä»˜ãå…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨ã‹ã‚„bounding boxesï¼‰ã‚’å¯èƒ½ã«ã—ã€ã¾ãŸé«˜ç”»è³ªãªç”»åƒã®ç”Ÿæˆã‚‚åŒæ™‚ã«é”æˆã—ã¦ã„ã‚‹

## Introduction
æ—¢å­˜ã®diffusion model
- å°¤åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹ã«å±ã™ã‚‹
- GPUã«ã‚ˆã‚‹é›»åŠ›æ¶ˆè²»ãŒæ¿€ã—ã„ (e.g. 150-1000 V in 100 days)
ã“ã‚Œã«ã‚ˆã£ã¦ä»¥ä¸‹ã®çŠ¶æ³ã‚’ã‚‚ãŸã‚‰ã—ãŸ
1. GPUãŒãŸãã•ã‚“å¿…è¦
2. ã™ã§ã«å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ã‚‚ã€æ™‚é–“çš„ãƒ»é‡‘éŠ­çš„ã«è² æ‹…ãŒã‹ã‹ã‚‹ã€‚åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ã‚‚ã®ã§ã‚‚æœ€åˆã‹ã‚‰åŒã˜ã“ã¨ã‚’ã—ãªã„ã¨ã„ã‘ãªã„ (e.g. 25-1000 steps)
è¨“ç·´ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç”Ÿæˆï¼‰ã®ä¸¡æ–¹ã®è¤‡é›‘æ€§ã‚’æ’é™¤ã—ã¤ã¤ã€æ€§èƒ½ã¯ä½ä¸‹ã•ã›ãªã„ã‚ˆã†ã«ã§ããªã„ã‹ã¨ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³

### Departure to Latent Space
ã™ã§ã«pixelç©ºé–“ã§è¨“ç·´ã•ã‚ŒãŸdiffusion modelã‚’åˆ†æã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã‚‹
å­¦ç¿’ã®æ®µéš
1. perceptual compression: high-frequencryãªç´°ã‹ã„éƒ¨åˆ†ã‚’å–ã‚Šé™¤ãã€å°‘ã—æ„å‘³çš„ãªå¤‰åˆ†ã‚‚å­¦ç¿’ã™ã‚‹
2. semantic compression: ãƒ‡ãƒ¼ã‚¿ã®æ„å‘³çš„ã§ã€æ¦‚å¿µçš„ãªçµ„ã¿åˆã‚ã›ã‚’å­¦ç¿’ã™ã‚‹

å­¦ç¿’ã‚’2æ®µéšã«åˆ†ã‘ã‚‹
1. çŸ¥è¦šçš„ã«ãƒ‡ãƒ¼ã‚¿ç©ºé–“ã«ç­‰ã—ã„ä½æ¬¡å…ƒãªè¡¨ç¾ç©ºé–“ã‚’å‡ºåŠ›ã™ã‚‹autoencoderã‚’è¨“ç·´ã™ã‚‹
ãƒ¡ãƒ¢ï¼šå¤šåˆ†ã“ã“ã§å‡ºåŠ›ã•ã‚Œã‚‹ä½æ¬¡å…ƒç©ºé–“ã‚’latent spaceã¨å‘¼ã‚“ã§ã„ã‚‹
-> perceptual compression (Autoencoder)
2. DMsã‚’latent spaceã§å­¦ç¿’ã™ã‚‹ã¨ãã«ã¯ã€æ—¢å­˜ã®æ‰‹æ³•ãŒè¡Œã£ã¦ã„ãŸã‚ˆã†ãªè¶…éç©ºé–“åœ§ç¸®(è¨³ã—æ–¹ãŒåˆã£ã¦ã„ã‚‹ã‹ã¯ã‚ã‹ã‚‰ãªã„)ã‚’ä½¿ç”¨ã—ãªã„
ã“ã®ã‚ˆã†ã«è¤‡é›‘æ€§ã‚’æ¸›ã‚‰ã—ã¦latent spaceã‹ã‚‰ã®åŠ¹ç‡çš„ãªç”»åƒç”Ÿæˆã‚’ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹
ã“ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ç¾¤ã‚’LDMsã¨åä»˜ã‘ã‚‹
-> Latent diffusion (DMs in latent space)

åˆ©ç‚¹
- autoencoderã®å­¦ç¿’ã¯1åº¦ã§ã„ã„
- DMãŒè¨“ç·´çµæœã¯ä½¿ã„ã¾ã‚ã›ã‚‹
- ã¾ãŸã¯è¨ˆç®—çš„ã«é›£ã—ã„å•é¡Œã®ãŸã‚ã®æ¢ç´¢ã«ä½¿ãˆã‚‹ï¼ˆåˆæœŸãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’æŠ‘ãˆã‚‰ã‚Œã‚‹åˆ†ã€ã‚ˆã‚Šæ·±ãã¾ã§æ¢ç´¢ã§ãã‚‹ï¼‰

contributions
1. ãƒ”ãƒ¥ã‚¢ãªtransformer-basedãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã‚¹ã‚±ãƒ¼ãƒ«ã—ã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ãˆã‚‹
  - ä½æ¬¡å…ƒãªlatent spaceã§å‹•ãã®ã§ã‚ˆã‚Šå¿ å®Ÿã§ç´°ã‹ã„ç”»åƒãŒä½œã‚Œã‚‹
  - é«˜ç”»è³ªç”»åƒã‚’åŠ¹ç‡çš„ã«ä½œã‚Œã‚‹
2. pixelãƒ™ãƒ¼ã‚¹ã®æ—¢å­˜æ‰‹æ³•ã‚„ãƒ‡ãƒ¼ã‚¿ã¨åŒç­‰ã®æ€§èƒ½ã‚’ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã€æ¨è«–ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤å®Ÿç¾
3. encoder/decoderã¨score-basedã®äº‹å‰åˆ†å¸ƒã‚’åŒæ™‚ã«å­¦ç¿’ã™ã‚‹ã¨ã„ã†ã“ã¨ã¯ã—ãªã„ï¼ˆã‚ã‘ã¦ã„ã‚‹ï¼‰ã®ã§ã€ç¹Šç´°ãªä½œæ¥­ãŒå¿…è¦ãªã„
4. $~1024^2 $ pxã®é«˜ç”»è³ªç”ŸæˆãŒã§ãã‚‹
5. cross-attentionãƒ™ãƒ¼ã‚¹ã®æ¡ä»¶ä»˜ãã®å­¦ç¿’ã‚‚ã§ãã‚‹
6. ä½¿ã„ã¾ã‚ã›ã‚‹ã‚ˆã†ã«ãªã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ï¼ˆã“ã‚Œã¾ã§ã¯å…¬é–‹ã®ã—ã‚ˆã†ãŒãªã‹ã£ãŸï¼Ÿï¼‰

## 2. Related work
æ™‚é–“ãŒãªã„ã®ã§ä¸€åº¦é£›ã°ã™

## 3. Method
- è¨ˆç®—ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ã¨ã„ã†å¼±ç‚¹ã‚’å…‹æœã™ã‚‹ãŸã‚ã€æ˜ç¤ºçš„ã«ç”Ÿæˆã®å­¦ç¿’éç¨‹ã‚’åˆ†ã‘ã‚‹
  - çŸ¥è¦šçš„ã«ç”»åƒç©ºé–“ã¨ç­‰ã—ã„ç©ºé–“ã‚’å­¦ç¿’ã™ã‚‹autoencoderã‚’æ´»ç”¨ã™ã‚‹
åŠ¹æœ
1. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç”Ÿæˆï¼‰ãŒä½æ¬¡å…ƒç©ºé–“ã§è¡Œã‚ã‚Œã‚‹ã®ã§è¨ˆç®—åŠ¹ç‡ã®è‰¯ã„DMãŒå¾—ã‚‰ã‚Œã‚‹
2. UNetç”±æ¥ã®inductive biasã‚’åˆ©ç”¨ã™ã‚‹ï¼ˆã‚ˆãã‚ã‹ã£ã¦ã„ãªã„ï¼‰
3. general-purposeãªåœ§ç¸®ãƒ¢ãƒ‡ãƒ«ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚ãã®æ½œåœ¨ç©ºé–“ã¯ã„ã‚ã‚“ãªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚„å¿œç”¨ã«ä½¿ãˆã‚‹




## 3.1 Perceptual Image Compression
perceptual compression modelã¯
- transformer (Patrick Esser, Robin Rombach, and BjÂ¨ orn Ommer. Taming
transformers for high-resolution image synthesis. CoRR,
abs/2012.09841, 2020.)
- autoencoder (perceptual lossã‚’ä½¿ã†)
- patch-based adversarial objective
ã®çµ„ã¿åˆã‚ã›ã€‚

1) Train an autoencoder:
Encoder: 
E(x)â†’z where $\mathbf{x} \in \mathcal{R}^{H\times W \times 3}$, $z \in \mathcal{R}^{h \times w \times c}$
Decoder: 
D(z)â†’$\tilde{x}$
The encoder downsamples by factor:
f = $2^m$
So HÃ—W collapses to (H/f) Ã— (W/f).
The latent has channels c but far fewer pixels.
ğŸ‘‰ The goal is to remove only imperceptible detail (â€œperceptual compressionâ€), while preserving all visible / semantic content.

2) Why perceptual loss + patch-GAN?
The paper uses:
- VGG perceptual loss (LPIPS-style)
- Patch-based discriminator (GAN loss)
This ensures:
- No blurriness (common with L2 autoencoders)
- Reconstructions stay on the â€œimage manifoldâ€
- Local texture realism is preserved
Pixel-space L1/L2 alone â†’ blurry reconstructions.
Perceptual + patch-GAN â†’ sharp, natural reconstructions.
